{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti1MYxJ8pxOL",
        "outputId": "1bf3c47d-13ac-4fac-cdfc-ea3c5dfdc464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.21.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.25.1-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.6.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.21.0 trio-0.25.1 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 selenium pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Tl6yGjT1p6BP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import logging\n",
        "import warnings\n",
        "import argparse\n",
        "import bs4 as bs\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def scrap(url, N_SCROLL, output_path, to_csv=True):\n",
        "    options = webdriver.ChromeOptions()\n",
        "    # options.add_argument('--headless')\n",
        "    options.add_argument(\"--lang=id\")\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    logging.info(\"opening url..\")\n",
        "    driver.get(url)\n",
        "\n",
        "    # terbaru\n",
        "    logging.info(\"click terbaru..\")\n",
        "    wait = WebDriverWait(driver, 10)\n",
        "    menu = wait.until(lambda driver: driver.find_element( 'xpath',\n",
        "        '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[7]/div[2]/button'))\n",
        "    menu.click()\n",
        "    terbaru = wait.until(lambda driver: driver.find_element('xpath',\n",
        "        '//*[@id=\"action-menu\"]/div[2]'))\n",
        "    terbaru.click()\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "    # scroll down\n",
        "    logging.info(\"scrolling down..\")\n",
        "    scrollable_div = driver.find_element(\"css selector\",\n",
        "        '#QA0Szd > div > div > div.w6VYqd > div.bJzME.tTVLSc > div > div.e07Vkf.kA9KIf > div > div > div.m6QErb.DxyBCb.kA9KIf.dS8AEf'\n",
        "    )\n",
        "\n",
        "    for _ in tqdm(range(int(N_SCROLL))):\n",
        "\n",
        "        driver.execute_script(\n",
        "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
        "            scrollable_div\n",
        "        )\n",
        "        time.sleep(1)\n",
        "\n",
        "    # Scraping\n",
        "\n",
        "    logging.info(\"get html..\")\n",
        "    resp = bs.BeautifulSoup(driver.page_source, 'lxml')\n",
        "    reviews = resp.select(\n",
        "        \"#QA0Szd > div > div > div.w6VYqd > div.bJzME.tTVLSc > div > div.e07Vkf.kA9KIf > div > div > div.m6QErb.DxyBCb.kA9KIf.dS8AEf > div:nth-child(9)\")\n",
        "    soup_reviews = bs.BeautifulSoup(str(reviews), 'lxml')\n",
        "    review = soup_reviews.find_all('div', class_='jftiEf fontBodyMedium')\n",
        "\n",
        "    # Nama\n",
        "    logging.info(\"get nama..\")\n",
        "    names = []\n",
        "    for i in range(len(review)):\n",
        "        names.append(review[i]['aria-label'])\n",
        "\n",
        "    # Bintang\n",
        "    logging.info(\"get bintang..\")\n",
        "    soup_bapak_bintang = bs.BeautifulSoup(str(review), 'lxml')\n",
        "    bapak_bintang = soup_bapak_bintang.find_all('div', class_='DU9Pgb')\n",
        "\n",
        "    bintangs = []\n",
        "    for i in range(len(bapak_bintang)):\n",
        "        soup_bintang = bs.BeautifulSoup(str(bapak_bintang[i]), 'lxml')\n",
        "        bintang = soup_bintang.find('span', class_='kvMYJc')\n",
        "        bintangs.append(int(bintang['aria-label'].split()[0]))\n",
        "\n",
        "    # Komentar\n",
        "    logging.info(\"get komentar..\")\n",
        "    komens = []\n",
        "    for i in range(len(review)):\n",
        "        try:\n",
        "            komen = bs.BeautifulSoup(str(review[i]), 'lxml').find('div', class_=\"MyEned\").text\n",
        "        except:\n",
        "            komen = \"\"\n",
        "        komens.append(komen)\n",
        "\n",
        "    data = pd.DataFrame({\n",
        "        \"nama\": names,\n",
        "        \"bintang\": bintangs,\n",
        "        \"komentar\": komens\n",
        "    })\n",
        "\n",
        "    logging.info(\"to csv..\")\n",
        "    if to_csv:\n",
        "        data.to_csv(output_path, index=False)\n",
        "    else:\n",
        "        return data"
      ],
      "metadata": {
        "id": "ppaSjB5ds-IL"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}